{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "import glob\n",
    "from scipy.spatial.transform import Rotation\n",
    "import numpy as np\n",
    "import laspy\n",
    "import os\n",
    "from scipy.spatial.transform import Rotation\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feather(path):\n",
    "    print(pd.read_feather(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_ns</th>\n",
       "      <th>qw</th>\n",
       "      <th>qx</th>\n",
       "      <th>qy</th>\n",
       "      <th>qz</th>\n",
       "      <th>tx_m</th>\n",
       "      <th>ty_m</th>\n",
       "      <th>tz_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>315968518660281000</td>\n",
       "      <td>0.999881</td>\n",
       "      <td>0.005417</td>\n",
       "      <td>-0.009145</td>\n",
       "      <td>-0.011189</td>\n",
       "      <td>4258.18925</td>\n",
       "      <td>1104.579802</td>\n",
       "      <td>-22.327265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         timestamp_ns        qw        qx        qy        qz        tx_m  \\\n",
       "7  315968518660281000  0.999881  0.005417 -0.009145 -0.011189  4258.18925   \n",
       "\n",
       "          ty_m       tz_m  \n",
       "7  1104.579802 -22.327265  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_feather('/mnt/d/argoverse/ff8e7fdb-1073-3592-ba5e-8111bc3ce48b/city_SE3_egovehicle.feather')\n",
    "b = a['timestamp_ns']\n",
    "a[b==315968518660281000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(692, 1276)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('/mnt/d/argoverse/ff8e7fdb-1073-3592-ba5e-8111bc3ce48b/map/ff8e7fdb-1073-3592-ba5e-8111bc3ce48b_ground_height_surface____WDC.npy').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             timestamp_ns                            track_uuid  \\\n",
      "0      315968518660281000  8974c6f5-3ad1-45be-a2b0-0bc254a8718b   \n",
      "1      315968518660281000  f126db57-7f76-41cf-90a9-bb1617dcc807   \n",
      "2      315968518660281000  1f8eb246-2724-4efa-a81e-05a2b1b96130   \n",
      "3      315968518660281000  4219cac3-b0cc-4c78-bac5-4ae44703c49e   \n",
      "4      315968518660281000  edc5c0df-cd13-4376-8ecc-f6aea4637662   \n",
      "...                   ...                                   ...   \n",
      "18500  315968538260009000  e4f4531f-fd14-414b-9afa-650bfded39e5   \n",
      "18501  315968538260009000  f6e97bd2-5f9b-4eb7-aa21-ec7ad9b88fb4   \n",
      "18502  315968538260009000  f739b1f3-8207-4836-9ee1-dfef158993fb   \n",
      "18503  315968538260009000  fcfa30be-b0fe-469a-809f-cd923c645a25   \n",
      "18504  315968538260009000  cba7a9ae-d201-42db-90e6-e6d3fe6df569   \n",
      "\n",
      "              category   length_m   width_m  height_m        qw   qx   qy  \\\n",
      "0              BICYCLE   1.500000  0.500000  1.025743 -0.004370  0.0  0.0   \n",
      "1                  BUS  12.280141  2.906546  3.682413 -0.003363  0.0  0.0   \n",
      "2                  DOG   0.854500  0.318100  0.454500  0.999994  0.0  0.0   \n",
      "3                  DOG   1.104009  0.318100  0.893267  0.999891  0.0  0.0   \n",
      "4        LARGE_VEHICLE   8.337326  2.877488  3.398233  0.999972  0.0  0.0   \n",
      "...                ...        ...       ...       ...       ...  ...  ...   \n",
      "18500  REGULAR_VEHICLE   4.727413  1.740000  1.731197  0.246590  0.0  0.0   \n",
      "18501  REGULAR_VEHICLE   4.030000  1.740000  1.626711  1.000000  0.0  0.0   \n",
      "18502  REGULAR_VEHICLE   4.030000  2.031569  1.410000  0.998172  0.0  0.0   \n",
      "18503  REGULAR_VEHICLE   4.368489  1.759190  1.601460 -0.022564  0.0  0.0   \n",
      "18504   WHEELED_DEVICE   1.078700  0.479100  1.226797  0.691967  0.0  0.0   \n",
      "\n",
      "             qz        tx_m       ty_m      tz_m  num_interior_pts  \n",
      "0      0.999990   -5.383209  -5.328548  0.405909                48  \n",
      "1      0.999994  123.209478   6.362516  0.951848                17  \n",
      "2      0.003356   -3.659448  -6.324487  0.159382                 2  \n",
      "3      0.014792   -3.818717  -7.401864  0.430312                52  \n",
      "4     -0.007515  130.452488  -3.437272  0.825996                43  \n",
      "...         ...         ...        ...       ...               ...  \n",
      "18500  0.969120  -14.101926  34.091392  0.169929               197  \n",
      "18501  0.000577 -113.492780   3.718781  0.622342                 2  \n",
      "18502 -0.060440   78.887344   4.096324  0.594779                 8  \n",
      "18503  0.999745 -152.731182  11.079188  0.281829                 1  \n",
      "18504  0.721929  -59.540066  -2.650765  0.739479                 1  \n",
      "\n",
      "[18505 rows x 14 columns]\n",
      "            timestamp_ns        qw        qx        qy        qz         tx_m  \\\n",
      "0     315968518622412932  0.999883  0.005024 -0.009153 -0.011220  4257.824876   \n",
      "1     315968518627482492  0.999882  0.005025 -0.009163 -0.011272  4257.872677   \n",
      "2     315968518637425437  0.999882  0.005175 -0.009123 -0.011241  4257.972406   \n",
      "3     315968518642441187  0.999882  0.005228 -0.009123 -0.011219  4258.019834   \n",
      "4     315968518649927209  0.999881  0.005285 -0.009147 -0.011233  4258.090516   \n",
      "...                  ...       ...       ...       ...       ...          ...   \n",
      "3357  315968538542441185  0.999963  0.003410 -0.001405 -0.007769  4440.243890   \n",
      "3358  315968538549927216  0.999963  0.003423 -0.001402 -0.007737  4440.309091   \n",
      "3359  315968538549927223  0.999963  0.003423 -0.001402 -0.007737  4440.309091   \n",
      "3360  315968538557428270  0.999963  0.003439 -0.001419 -0.007711  4440.374372   \n",
      "3361  315968538562451239  0.999964  0.003447 -0.001436 -0.007662  4440.417665   \n",
      "\n",
      "             ty_m       tz_m  \n",
      "0     1104.588014 -22.330420  \n",
      "1     1104.587221 -22.329489  \n",
      "2     1104.584787 -22.330714  \n",
      "3     1104.583669 -22.329866  \n",
      "4     1104.582051 -22.328558  \n",
      "...           ...        ...  \n",
      "3357  1097.431284 -20.325766  \n",
      "3358  1097.430742 -20.325674  \n",
      "3359  1097.430742 -20.325674  \n",
      "3360  1097.430173 -20.325775  \n",
      "3361  1097.429583 -20.325530  \n",
      "\n",
      "[3362 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# print_feather('/mnt/d/argoverse/ff8e7fdb-1073-3592-ba5e-8111bc3ce48b/calibration/intrinsics.feather')\n",
    "# print_feather('/mnt/d/argoverse/SIBH0EdRvYcnGrKNEOzl469Y9Qbx690o/city_SE3_egovehicle.feather')\n",
    "# print_feather('/mnt/d/argoverse/SIBH0EdRvYcnGrKNEOzl469Y9Qbx690o/calibration/egovehicle_SE3_sensor.feather')\n",
    "# print_feather('/mnt/d/argoverse/ff8e7fdb-1073-3592-ba5e-8111bc3ce48b/sensors/lidar/315968518660281000.feather')\n",
    "print_feather('/mnt/d/argoverse/ff8e7fdb-1073-3592-ba5e-8111bc3ce48b/annotations.feather')\n",
    "print_feather('/mnt/d/argoverse/ff8e7fdb-1073-3592-ba5e-8111bc3ce48b/city_SE3_egovehicle.feather')\n",
    "# print_feather('/mnt/d/argoverse/eec284b2-840a-3c75-aa42-04d2e309bbe1/sensors/lidar/315972314959811000.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = False\n",
    "\n",
    "POINT_ATTR = {\n",
    "    'X': 0,\n",
    "    'Y': 1,\n",
    "    'Z': 2,\n",
    "    'INTENSITY': 3,\n",
    "    'GPS': 4,\n",
    "    'INDEX': 5,\n",
    "    'R':6,\n",
    "    'G':7,\n",
    "    'B':8,\n",
    "    'CATEGORY':9\n",
    "}\n",
    "\n",
    "SENSOR_TYPE = {\n",
    "    'CAMERA': 0,\n",
    "    'LIDAR': 1\n",
    "}\n",
    "\n",
    "SENSOR = {\n",
    "    'FC': {'id': 0, 'name':'ring_front_center', 'type':SENSOR_TYPE['CAMERA']},\n",
    "    'FL': {'id': 1, 'name':'ring_front_left', 'type':SENSOR_TYPE['CAMERA']},\n",
    "    'FR': {'id': 2, 'name':'ring_front_right', 'type':SENSOR_TYPE['CAMERA']},\n",
    "    'RL': {'id': 3, 'name':'ring_rear_left', 'type':SENSOR_TYPE['CAMERA']},\n",
    "    'RR': {'id': 4, 'name':'ring_rear_right', 'type':SENSOR_TYPE['CAMERA']},\n",
    "    'SL': {'id': 5, 'name':'ring_side_left', 'type':SENSOR_TYPE['CAMERA']},\n",
    "    'SR': {'id': 6, 'name':'ring_side_right', 'type':SENSOR_TYPE['CAMERA']},\n",
    "    'SFL': {'id': 7, 'name':'stereo_front_left', 'type':SENSOR_TYPE['CAMERA']},\n",
    "    'SFR': {'id': 8, 'name':'stereo_front_right', 'type':SENSOR_TYPE['CAMERA']},\n",
    "    'UL': {'id': 9, 'name':'up_lidar', 'type':SENSOR_TYPE['LIDAR']},\n",
    "    'DL': {'id': 10, 'name':'down_lidar', 'type':SENSOR_TYPE['LIDAR']},\n",
    "}\n",
    "\n",
    "CATEGORY = {\n",
    "    'NONE': 0,\n",
    "    'REGULAR_VEHICLE': 1,\n",
    "    'PEDESTRIAN': 2,\n",
    "    'BOLLARD': 3,\n",
    "    'CONSTRUCTION_CONE': 3,\n",
    "    'CONSTRUCTION_BARREL': 3,\n",
    "    'STOP_SIGN': 4,\n",
    "    'BICYCLE': 1,\n",
    "    'LARGE_VEHICLE': 1,\n",
    "    'WHEELED_DEVICE': 1,\n",
    "    'BUS': 1,\n",
    "    'BOX_TRUCK': 1,\n",
    "    'SIGN': 5,\n",
    "    'TRUCK': 1,\n",
    "    'MOTORCYCLE': 1,\n",
    "    'BICYCLIST': 2,\n",
    "    'VEHICULAR_TRAILER': 1,\n",
    "    'TRUCK_CAB': 1,\n",
    "    'MOTORCYCLIST': 2,\n",
    "    'DOG': 2,\n",
    "    'SCHOOL_BUS': 1,\n",
    "    'WHEELED_RIDER': 2,\n",
    "    'STROLLER': 1,\n",
    "    'ARTICULATED_BUS': 1,\n",
    "    'MESSAGE_BOARD_TRAILER': 1,\n",
    "    'MOBILE_PEDESTRIAN_SIGN': 5,\n",
    "    'WHEELCHAIR': 1,\n",
    "    'RAILED_VEHICLE': 1,\n",
    "    'OFFICIAL_SIGNALER': 2,\n",
    "    'TRAFFIC_LIGHT_TRAILER': 1,\n",
    "    'ANIMAL': 2\n",
    "}\n",
    "\n",
    "MIN_DISTANCE = 1.0\n",
    "MAX_DISTANCE = 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_timestamp_from_path(path:str)->int:\n",
    "    filename = os.path.basename(path)\n",
    "    return int(filename.split('.')[0])\n",
    "\n",
    "def get_rotation_matrix(w:float, x:float, y:float, z:float)->np.ndarray:\n",
    "    return Rotation([x, y, z, w]).as_matrix()\n",
    "\n",
    "def get_transform_matrix(w:float, x:float, y:float, z:float, tx:float, ty:float, tz:float)->np.ndarray:\n",
    "    mat = np.eye(4)\n",
    "    mat[:3,:3] = get_rotation_matrix(w, x, y, z)\n",
    "    mat[:,3] = [tx, ty, tz, 1.0]\n",
    "    \n",
    "    return mat\n",
    "    \n",
    "\n",
    "def transform_points(points:np.ndarray, calibration:np.ndarray):\n",
    "    size = points.shape[0]\n",
    "    coords = np.concatenate((points[:,:3].T, np.ones((1, size))))\n",
    "    coords = np.matmul(calibration, coords)\n",
    "    \n",
    "    return coords[:3,:].T\n",
    "\n",
    "def image_to_numpy(path:str):\n",
    "    image = Image.open(path)\n",
    "    np_image = np.array(image)\n",
    "    \n",
    "    return np_image.transpose(2, 0, 1)\n",
    "\n",
    "# transform points from world to camera pixel\n",
    "def point_to_pixel(points:np.ndarray, to_sensor_calib:np.ndarray, intrinsic:np.ndarray):\n",
    "    coords = transform_points(points, to_sensor_calib)\n",
    "    size = coords.shape[0]\n",
    "    depth = coords[:,2]\n",
    "    \n",
    "    pixel = np.matmul(intrinsic, coords.T)\n",
    "    pixel = pixel / pixel[2, :]\n",
    "\n",
    "    return pixel.T, depth\n",
    "\n",
    "def get_rgb_bilinear(img:np.ndarray, pixel:np.ndarray):\n",
    "    x = pixel[:,0]\n",
    "    y= pixel[:,1]\n",
    "    \n",
    "    x0 = np.floor(x).astype(np.int32)\n",
    "    y0 = np.floor(y).astype(np.int32)\n",
    "    x1 = np.ceil(x).astype(np.int32)\n",
    "    y1 = np.ceil(y).astype(np.int32)\n",
    "\n",
    "    lt = img[:, y0, x0]\n",
    "    lb = img[:, y1, x0]\n",
    "    rt = img[:, y0, x1]\n",
    "    rb = img[:, y1, x1]\n",
    "    \n",
    "    wa = np.expand_dims(((x1.astype(np.float16) - x) * (y1.astype(np.float16) - y)), 0)\n",
    "    wb = np.expand_dims(((x1.astype(np.float16) - x) * (y - y0.astype(np.float16))), 0)\n",
    "    wc = np.expand_dims(((x - x0.astype(np.float16)) * (y1.astype(np.float16) - y)), 0)\n",
    "    wd = np.expand_dims(((x - x0.astype(np.float16)) * (y - y0.astype(np.float16))), 0)\n",
    "    \n",
    "    return (lt * wa + lb * wb + rt * wc + rb * wd).transpose(1, 0)\n",
    "\n",
    "def get_color_from_image(image:str, pixel:np.ndarray, depth:np.ndarray):\n",
    "    np_image = image_to_numpy(image)\n",
    "    _, h, w = np_image.shape\n",
    "    size = pixel.shape[0]\n",
    "    \n",
    "    x = pixel[:, 0]\n",
    "    y = pixel[:, 1]\n",
    "    mask = np.ones(size, dtype=bool)\n",
    "    mask = np.logical_and(mask, depth > 1.0)\n",
    "    mask = np.logical_and(mask, x > 1)\n",
    "    mask = np.logical_and(mask, x < w - 1)\n",
    "    mask = np.logical_and(mask, y > 1)\n",
    "    mask = np.logical_and(mask, y < h - 1)\n",
    "    rgb = get_rgb_bilinear(np_image, pixel[mask,:2])\n",
    "    \n",
    "    return rgb, mask\n",
    "\n",
    "def draw_points_on_image(image:str, pixel:np.ndarray):\n",
    "    np_image = image_to_numpy(image).transpose(1, 2, 0)\n",
    "    x = pixel[:,0]\n",
    "    y = pixel[:,1]\n",
    "    \n",
    "    plt.imshow(np_image)\n",
    "    plt.scatter(x, y, c='blue', marker='x', s=1)\n",
    "    plt.show()\n",
    "    \n",
    "def write_las(path:str, points:np.ndarray):\n",
    "    header = laspy.LasHeader(point_format=3, version=\"1.4\")\n",
    "    header.offsets = np.min(points[:,:3], axis=0)\n",
    "    header.scales = np.array([0.01, 0.01, 0.01])\n",
    "    # header.vlrs\n",
    "    \n",
    "    points_type = points.shape[1]\n",
    "    \n",
    "    las = laspy.LasData(header)\n",
    "    las.x = points[:,0]\n",
    "    las.y = points[:,1]\n",
    "    las.z = points[:,2]\n",
    "    las.intensity = points[:,POINT_ATTR['INTENSITY']]\n",
    "    if points_type >= POINT_ATTR['GPS']:\n",
    "        las.gps_time = points[:,POINT_ATTR['GPS']].astype(np.uint64)\n",
    "    \n",
    "    if points_type >= POINT_ATTR['B']:\n",
    "        las.red = points[:,POINT_ATTR['R']].astype(np.uint8)\n",
    "        las.green = points[:,POINT_ATTR['G']].astype(np.uint8)\n",
    "        las.blue = points[:,POINT_ATTR['B']].astype(np.uint8)\n",
    "    if points_type >= POINT_ATTR['CATEGORY']:\n",
    "        las.classification = points[:,POINT_ATTR['CATEGORY']].astype(np.uint8)\n",
    "    las.write(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EgoPoseParser():\n",
    "    def __init__(self, path:str):\n",
    "        feather = pd.read_feather(path)\n",
    "        \n",
    "        qw = feather['qw']\n",
    "        qx = feather['qx']\n",
    "        qy = feather['qy']\n",
    "        qz = feather['qz']\n",
    "        tx = feather['tx_m']\n",
    "        ty = feather['ty_m']\n",
    "        tz = feather['tz_m']\n",
    "        \n",
    "        self.timestamps = feather['timestamp_ns']\n",
    "        self.ego_calibrations = np.stack(\n",
    "            [get_transform_matrix(qw[i],qx[i],qy[i],qz[i],tx[i],ty[i],tz[i]) for i in range(len(self.timestamps))], axis=0\n",
    "        )\n",
    "    \n",
    "    def get_ego_calibration(self, timestamp: int)->np.ndarray:\n",
    "        return self.ego_calibrations[self.timestamps == timestamp].squeeze()\n",
    "                \n",
    "    \n",
    "class CalibrationParser():\n",
    "    def __init__(self, path:str):\n",
    "        feather = pd.read_feather(path)\n",
    "        \n",
    "        qw = feather['qw']\n",
    "        qx = feather['qx']\n",
    "        qy = feather['qy']\n",
    "        qz = feather['qz']\n",
    "        tx = feather['tx_m']\n",
    "        ty = feather['ty_m']\n",
    "        tz = feather['tz_m']\n",
    "        \n",
    "        self.sensor_calibrations = np.stack(\n",
    "            [get_transform_matrix(qw[i],qx[i],qy[i],qz[i],tx[i],ty[i],tz[i]) for i in range(len(qw))], axis=0\n",
    "        )\n",
    "    \n",
    "    def get_sensor_calibration(self, sensor_id:int)->np.ndarray:\n",
    "        return self.sensor_calibrations[sensor_id].squeeze()\n",
    "        \n",
    "\n",
    "class IntrinsicParser():\n",
    "    def __init__(self, path:str) -> None:\n",
    "        feather = pd.read_feather(path)\n",
    "        \n",
    "        fx_px = feather['fx_px']\n",
    "        fy_px = feather['fy_px']\n",
    "        cx_px = feather['cx_px']\n",
    "        cy_px = feather['cy_px']\n",
    "        \n",
    "        self.intrinsics = np.stack([np.eye(3) for _ in range(len(fx_px))], axis=0)\n",
    "        self.intrinsics[:, 0, 0] = fx_px\n",
    "        self.intrinsics[:, 1, 1] = fy_px\n",
    "        self.intrinsics[:, 0, 2] = cx_px\n",
    "        self.intrinsics[:, 1, 2] = cy_px\n",
    "    \n",
    "    def get_intrinsic(self, sensor_id:int)->np.ndarray:\n",
    "        return self.intrinsics[sensor_id]\n",
    "\n",
    "\n",
    "\n",
    "class Annotation():\n",
    "    def __init__(self, category:int, length:float, width:float, height:float, transform_mat:np.ndarray, inner_points:int) -> None:\n",
    "        self.category = category\n",
    "        self.length = length/2 + 5e-2\n",
    "        self.width = width/2 + 5e-2\n",
    "        self.height = height/2 + 5e-2\n",
    "        self.transform_mat = np.linalg.inv(transform_mat)\n",
    "        self.inner_points = inner_points\n",
    "    \n",
    "    def classify_points(self, points:np.ndarray) -> None:\n",
    "        size = points.shape[0]\n",
    "        coords = points[:,:3]\n",
    "        coords = transform_points(coords, self.transform_mat)\n",
    "        \n",
    "        xs = coords[:,0]\n",
    "        ys = coords[:,1]\n",
    "        zs = coords[:,2]\n",
    "        \n",
    "        mask = xs >= -self.length\n",
    "        mask = np.logical_and(mask, xs<=self.length)\n",
    "        mask = np.logical_and(mask, ys>=-self.width)\n",
    "        mask = np.logical_and(mask, ys<=self.width)\n",
    "        mask = np.logical_and(mask, zs>=-self.height)\n",
    "        mask = np.logical_and(mask, zs<=self.height)\n",
    "        \n",
    "        categories = np.zeros((size))\n",
    "        categories[mask] = self.category\n",
    "        \n",
    "        diff = self.inner_points - categories[mask].shape[0]\n",
    "        if diff > 1:\n",
    "            print(self.category, diff)\n",
    "        \n",
    "        return categories\n",
    "        \n",
    "\n",
    "class AnnotationParser():\n",
    "    def __init__(self, path:str) -> None:\n",
    "        feather = pd.read_feather(path)\n",
    "        \n",
    "        timestamps = feather['timestamp_ns']\n",
    "        categories = feather['category']\n",
    "        lengths = feather['length_m']\n",
    "        widths = feather['width_m']\n",
    "        heights = feather['height_m']\n",
    "        qw = feather['qw']\n",
    "        qx = feather['qx']\n",
    "        qy = feather['qy']\n",
    "        qz = feather['qz']\n",
    "        tx = feather['tx_m']\n",
    "        ty = feather['ty_m']\n",
    "        tz = feather['tz_m']\n",
    "        inner_points = feather['num_interior_pts']\n",
    "        \n",
    "        num_data = len(timestamps)\n",
    "        transform_matrix = [get_transform_matrix(qw[i],qx[i],qy[i],qz[i],tx[i],ty[i],tz[i]) for i in range(num_data)]\n",
    "        \n",
    "        self.annotations = {}\n",
    "        for i in range(num_data):\n",
    "            timestamp = timestamps[i]\n",
    "            if not timestamp in self.annotations:\n",
    "                self.annotations[timestamp] = []\n",
    "            self.annotations[timestamp].append(Annotation(CATEGORY[categories[i]], lengths[i], widths[i], heights[i], transform_matrix[i], inner_points[i]))\n",
    "            \n",
    "    def get_annotations(self, timestamp:int)->list[Annotation]:\n",
    "        return self.annotations[timestamp]\n",
    "        \n",
    "\n",
    "\n",
    "class Lidar():\n",
    "    def __init__(self, timestamp:int, path:str, ego_calibration:np.ndarray, ul_calibration:np.ndarray, dl_calibration:np.ndarray, annotations:list[Annotation]):\n",
    "        self.timestamp = timestamp\n",
    "        self.path = path\n",
    "        self.ego_calibration = ego_calibration\n",
    "        self.ul_calibration = ul_calibration\n",
    "        self.dl_calibration = dl_calibration\n",
    "        self.annotations = annotations\n",
    "        self.is_init = False\n",
    "        \n",
    "    def init(self)->None:\n",
    "\n",
    "        feather = pd.read_feather(self.path)\n",
    "        xs = feather['x']\n",
    "        ys = feather['y']\n",
    "        zs = feather['z']\n",
    "\n",
    "        coordinates = np.stack([xs, ys, zs], axis=1)\n",
    "        laser_number = feather['laser_number']\n",
    "        intensities = feather['intensity']\n",
    "        gps_times = feather['offset_ns']\n",
    "        \n",
    "        # # distance filter\n",
    "        # distance = np.sum(np.square(coordinates), axis=1)\n",
    "        # distance_filter = distance <= MAX_DISTANCE**2\n",
    "        \n",
    "        # coordinates = coordinates[distance_filter]\n",
    "        # laser_number = laser_number[distance_filter]\n",
    "        # intensities = intensities[distance_filter]\n",
    "        # gps_times = gps_times[distance_filter]\n",
    "        \n",
    "        num_points = coordinates.shape[0]\n",
    "        categories = np.zeros((num_points))\n",
    "        point_index = np.arange(num_points)\n",
    "        \n",
    "        \n",
    "        # transform coordinates from lidar to world\n",
    "        up_index = laser_number[laser_number >= 32]\n",
    "        down_index = laser_number[laser_number < 32]\n",
    "        # lidar to ego vehicle\n",
    "        coordinates[up_index] = transform_points(coordinates[up_index], self.ul_calibration)\n",
    "        coordinates[down_index] = transform_points(coordinates[down_index], self.dl_calibration)\n",
    "        \n",
    "        # classify points with annotations\n",
    "        for annotation in self.annotations:\n",
    "            classified = annotation.classify_points(coordinates)\n",
    "            valid = classified > 0\n",
    "            categories[valid] = classified[valid]\n",
    "\n",
    "        # ego vehicle to world\n",
    "        coordinates = transform_points(coordinates, self.ego_calibration)\n",
    "        \n",
    "        self.coordinates = coordinates\n",
    "        self.intensities = intensities\n",
    "        self.gps_times = gps_times + self.timestamp\n",
    "        self.categories = categories\n",
    "        self.point_index = point_index\n",
    "        self.is_init = True\n",
    "    \n",
    "    # x, y, z, intensity, gps_time, point-index\n",
    "    def get_points(self)->np.ndarray:\n",
    "        if not self.is_init:\n",
    "            self.init()\n",
    "        \n",
    "        num_points = self.coordinates.shape[0]\n",
    "        points = np.zeros((num_points, len(POINT_ATTR)))\n",
    "        points[:,:3] = self.coordinates\n",
    "        points[:,POINT_ATTR['INTENSITY']] = self.intensities\n",
    "        points[:,POINT_ATTR['GPS']] = self.gps_times\n",
    "        points[:,POINT_ATTR['INDEX']] = self.point_index\n",
    "        points[:,POINT_ATTR['CATEGORY']] = self.categories\n",
    "        \n",
    "        return points\n",
    "\n",
    "class Camera():\n",
    "    def __init__(self, timestamp:int, images:list[str], ego_calibration:np.ndarray) -> None:\n",
    "        self.timestamp = timestamp\n",
    "        self.images = images\n",
    "        self.ego_calibraion = ego_calibration\n",
    "    \n",
    "\n",
    "class Argoverse():\n",
    "    def __init__(self, root:str) -> None:\n",
    "        self.root = root\n",
    "        \n",
    "        self.sensor_calibrations = CalibrationParser(join(root, 'calibration', 'egovehicle_SE3_sensor.feather'))\n",
    "        self.intrinsics = IntrinsicParser(join(root, 'calibration', 'intrinsics.feather'))\n",
    "        \n",
    "        annotations = AnnotationParser(join(root, 'annotations.feather'))\n",
    "        ego_poses = EgoPoseParser(join(root, 'city_SE3_egovehicle.feather'))\n",
    "        self.sensors = self.sync_lidar_camera(self.parse_lidars(ego_poses, annotations), self.parse_cameras(ego_poses))\n",
    "    \n",
    "    def parse_cameras(self, ego_poses:EgoPoseParser)->list[Camera]:\n",
    "        cameras = []\n",
    "        num_cams = SENSOR['SFL']['id']\n",
    "        image_files = [[] for _ in range(num_cams)]\n",
    "        \n",
    "        # get all jpg from each sensors dir\n",
    "        for _, item in SENSOR.items():\n",
    "            if item['id'] >= num_cams:\n",
    "                continue\n",
    "            image_files[item['id']] = glob.glob(join(self.root, 'sensors', 'cameras', item['name'], '*.jpg'))\n",
    "        \n",
    "        # create Camera instances\n",
    "        num_images = len(image_files[0])\n",
    "        for i in range(num_images):\n",
    "            timestamp = get_timestamp_from_path(image_files[0][i])\n",
    "            ego_calibration = ego_poses.get_ego_calibration(timestamp)\n",
    "            images= []\n",
    "            for j in range(num_cams):\n",
    "                images.append(image_files[j][i])\n",
    "            \n",
    "            cameras.append(Camera(timestamp, images, ego_calibration))\n",
    "        \n",
    "        return cameras\n",
    "\n",
    "        \n",
    "    def parse_lidars(self, ego_poses:EgoPoseParser, annotations:AnnotationParser)->list[Lidar]:\n",
    "        ul_calibration = self.sensor_calibrations.get_sensor_calibration([SENSOR['UL']['id']])\n",
    "        dl_calibration = self.sensor_calibrations.get_sensor_calibration([SENSOR['DL']['id']])\n",
    "        \n",
    "        lidar_files = glob.glob(join(self.root, 'sensors', 'lidar', '*.feather'))\n",
    "        lidars = []\n",
    "        for lidar_file in lidar_files:\n",
    "            timestamp = get_timestamp_from_path(lidar_file)\n",
    "            ego_calibration = ego_poses.get_ego_calibration(timestamp)\n",
    "            lidars.append(Lidar(timestamp, lidar_file, ego_calibration, ul_calibration, dl_calibration, annotations.get_annotations(timestamp)))\n",
    "            \n",
    "        return lidars\n",
    "    \n",
    "    def sync_lidar_camera(self, lidars:list[Lidar], cameras:list[Camera])->list(tuple((Lidar, Camera))):\n",
    "        sensors = []\n",
    "\n",
    "        for lidar in lidars:\n",
    "            timestamp = lidar.timestamp\n",
    "            \n",
    "            for camera in cameras:\n",
    "                if timestamp < camera.timestamp:\n",
    "                    sensors.append((lidar, camera))\n",
    "                    break\n",
    "        \n",
    "        return sensors\n",
    "\n",
    "    \n",
    "    def get_points(self)->np.ndarray:\n",
    "        num_points = 0\n",
    "        points_list = []\n",
    "        \n",
    "        for sensors in self.sensors:\n",
    "            lidar:Lidar = sensors[0]\n",
    "            camera:Camera = sensors[1]\n",
    "            \n",
    "            points = lidar.get_points()\n",
    "            points[:,POINT_ATTR['INDEX']] += num_points\n",
    "            num_points += points.shape[0]\n",
    "            \n",
    "            # colorize points\n",
    "            num_cams = SENSOR['SFL']['id']\n",
    "            colored_points = []\n",
    "            # world to ego_vehicle\n",
    "            coords = transform_points(points, np.linalg.inv(camera.ego_calibraion))\n",
    "            \n",
    "            for sensor_id in range(num_cams):\n",
    "                if points.shape[0] <= 0:\n",
    "                    break\n",
    "                \n",
    "                image = camera.images[sensor_id]\n",
    "                sensor_calib = self.sensor_calibrations.get_sensor_calibration(sensor_id)\n",
    "                intrinsic = self.intrinsics.get_intrinsic(sensor_id)\n",
    "                # project points into camera\n",
    "                pixel, depth = point_to_pixel(coords, np.linalg.inv(sensor_calib), intrinsic)\n",
    "                rgb, mask = get_color_from_image(image, pixel, depth)\n",
    "                points[mask,POINT_ATTR['R']:POINT_ATTR['R']+3] = rgb\n",
    "                \n",
    "                colored_points.append(points[mask])\n",
    "                points = points[np.logical_not(mask)]\n",
    "                coords = coords[np.logical_not(mask)]\n",
    "                \n",
    "                if DEBUG_MODE:\n",
    "                    draw_points_on_image(image, pixel[mask])\n",
    "            \n",
    "            points = np.concatenate(colored_points, 0)\n",
    "            points_list.append(points)\n",
    "        \n",
    "        return np.concatenate(points_list, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "argoverse = Argoverse('/mnt/d/argoverse/train/01bb304d-7bd8-35f8-bbef-7086b688e35e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n",
      "3 2\n",
      "3 3\n",
      "3 2\n",
      "3 2\n",
      "3 2\n",
      "3 2\n",
      "1 3\n",
      "1 4\n",
      "1 3\n",
      "1 4\n",
      "1 3\n",
      "2 2\n",
      "1 2\n",
      "1 2\n",
      "3 2\n",
      "1 2\n",
      "1 11\n",
      "1 3\n",
      "3 3\n",
      "3 2\n",
      "3 4\n",
      "3 5\n",
      "3 6\n",
      "1 9\n",
      "3 3\n",
      "3 2\n",
      "1 3\n",
      "1 2\n"
     ]
    }
   ],
   "source": [
    "points = argoverse.get_points()\n",
    "write_las('/mnt/d/argoverse/train/01bb304d-7bd8-35f8-bbef-7086b688e35e/test.las', points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_las('/mnt/d/argoverse/test/test.las', points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_points(points:torch.Tensor)->torch.Tensor:\n",
    "#     points[:,:,:3] -= torch.mean(points[:,:,:3], dim=1)\n",
    "#     points[:,:,:3] /= torch.max(torch.abs(points[:,:,:3]), dim=1).values\n",
    "#     return points\n",
    "\n",
    "# def create_las(path:str, argoverse:Argoverse, model)->None:\n",
    "#     points_list = []\n",
    "#     classes_list = []\n",
    "    \n",
    "#     for i in range(argoverse.num_index):\n",
    "#         points = argoverse.get_points(i)\n",
    "#         points_list.append(points)\n",
    "#         points = points.unsqueeze(0).cuda().type(torch.float32)\n",
    "#         points = normalize_points(points)\n",
    "#         points[:,:,3] = points[:,:,3] / 255.0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             prediction = model(points)\n",
    "#             prediction = prediction.transpose(1, 2)\n",
    "#             prediction = torch.nn.functional.softmax(prediction, dim=1)\n",
    "#             prediction = prediction.argmax(dim=1).squeeze().detach().cpu()\n",
    "#             print(prediction.shape, prediction[prediction > 0].shape)\n",
    "#             classes_list.append(prediction)\n",
    "            \n",
    "#     points = torch.concatenate(points_list, dim=0).numpy()\n",
    "#     classes = torch.concatenate(classes_list, dim=0).numpy()\n",
    "#     intensity = points[:, 3].astype(np.uint8)\n",
    "    \n",
    "#     header = laspy.LasHeader(point_format=3, version=\"1.4\")\n",
    "#     header.offsets = np.min(points[:,:3], axis=0)\n",
    "#     header.scales = np.array([0.01, 0.01, 0.01])\n",
    "#     # header.vlrs\n",
    "    \n",
    "#     las = laspy.LasData(header)\n",
    "#     las.x = points[:,0]\n",
    "#     las.y = points[:,1]\n",
    "#     las.z = points[:,2]\n",
    "#     las.intensity = intensity\n",
    "#     las.classification = classes\n",
    "#     # las.gps_time = paris.gps_time\n",
    "#     # las.point_source_id = paris.frame_index\n",
    "    \n",
    "#     las.write(path)\n",
    "#     print(f'{points.shape[0]} points were saved on {path}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vision.PointTransformer import PointTransformerSeg\n",
    "# num_classes = 5\n",
    "# seg_model = PointTransformerSeg(4, num_classes)\n",
    "# seg_model.load_state_dict(torch.load('data/point_transformer_seg.pt'))\n",
    "# seg_model.cuda()\n",
    "# seg_model.eval()\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argoverse = Argoverse('data/0dr6jn0kF6YjT9Qr1mtpYrE0ihkGpKsd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_las('./test.las', argoverse, seg_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py309",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
