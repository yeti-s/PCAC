{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "import glob\n",
    "from scipy.spatial.transform import Rotation\n",
    "import numpy as np\n",
    "import laspy\n",
    "import os\n",
    "from scipy.spatial.transform import Rotation\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = False\n",
    "VALID_HEIGHT_ONLY = False\n",
    "CLASSIFY_CATEGORY = False\n",
    "\n",
    "POINT_ATTR = {\n",
    "    'X': 0,\n",
    "    'Y': 1,\n",
    "    'Z': 2,\n",
    "    'INTENSITY': 3,\n",
    "    'GPS': 4,\n",
    "    'INDEX': 5,\n",
    "    'R':6,\n",
    "    'G':7,\n",
    "    'B':8,\n",
    "    'CATEGORY':9\n",
    "}\n",
    "\n",
    "SENSOR_TYPE = {\n",
    "    'CAMERA': 0,\n",
    "    'LIDAR': 1\n",
    "}\n",
    "\n",
    "SENSOR = {\n",
    "    'FC': {'id': 0, 'name':'ring_front_center', 'type':SENSOR_TYPE['CAMERA'], 'valid_height':[1125, 1825]},\n",
    "    'FL': {'id': 1, 'name':'ring_front_left', 'type':SENSOR_TYPE['CAMERA'], 'valid_height':[825, 1550]},\n",
    "    'FR': {'id': 2, 'name':'ring_front_right', 'type':SENSOR_TYPE['CAMERA'], 'valid_height':[825, 1550]},\n",
    "    'RL': {'id': 3, 'name':'ring_rear_left', 'type':SENSOR_TYPE['CAMERA'], 'valid_height':[885, 1285]},\n",
    "    'RR': {'id': 4, 'name':'ring_rear_right', 'type':SENSOR_TYPE['CAMERA'], 'valid_height':[885, 1285]},\n",
    "    'SL': {'id': 5, 'name':'ring_side_left', 'type':SENSOR_TYPE['CAMERA'], 'valid_height':[825, 1550]},\n",
    "    'SR': {'id': 6, 'name':'ring_side_right', 'type':SENSOR_TYPE['CAMERA'], 'valid_height':[825, 1550]},\n",
    "    'SFL': {'id': 7, 'name':'stereo_front_left', 'type':SENSOR_TYPE['CAMERA'], 'valid_height':[1125, 1825]},\n",
    "    'SFR': {'id': 8, 'name':'stereo_front_right', 'type':SENSOR_TYPE['CAMERA'], 'valid_height':[1125, 1825]},\n",
    "    'UL': {'id': 9, 'name':'up_lidar', 'type':SENSOR_TYPE['LIDAR']},\n",
    "    'DL': {'id': 10, 'name':'down_lidar', 'type':SENSOR_TYPE['LIDAR']},\n",
    "}\n",
    "\n",
    "CATEGORY = {\n",
    "    'NONE': 0,\n",
    "    'REGULAR_VEHICLE': 1,\n",
    "    'PEDESTRIAN': 2,\n",
    "    'BOLLARD': 3,\n",
    "    'CONSTRUCTION_CONE': 3,\n",
    "    'CONSTRUCTION_BARREL': 3,\n",
    "    'STOP_SIGN': 4,\n",
    "    'BICYCLE': 1,\n",
    "    'LARGE_VEHICLE': 1,\n",
    "    'WHEELED_DEVICE': 1,\n",
    "    'BUS': 1,\n",
    "    'BOX_TRUCK': 1,\n",
    "    'SIGN': 5,\n",
    "    'TRUCK': 1,\n",
    "    'MOTORCYCLE': 1,\n",
    "    'BICYCLIST': 2,\n",
    "    'VEHICULAR_TRAILER': 1,\n",
    "    'TRUCK_CAB': 1,\n",
    "    'MOTORCYCLIST': 2,\n",
    "    'DOG': 2,\n",
    "    'SCHOOL_BUS': 1,\n",
    "    'WHEELED_RIDER': 2,\n",
    "    'STROLLER': 1,\n",
    "    'ARTICULATED_BUS': 1,\n",
    "    'MESSAGE_BOARD_TRAILER': 1,\n",
    "    'MOBILE_PEDESTRIAN_CROSSING_SIGN': 5,\n",
    "    'WHEELCHAIR': 1,\n",
    "    'RAILED_VEHICLE': 1,\n",
    "    'OFFICIAL_SIGNALER': 2,\n",
    "    'TRAFFIC_LIGHT_TRAILER': 1,\n",
    "    'ANIMAL': 2\n",
    "}\n",
    "\n",
    "CITY_ORIGIN_LATLON = {\n",
    "    'ATX': (30.27464237939507, -97.7404457407424),\n",
    "    'DTW': (42.29993066912924, -83.17555750783717),\n",
    "    'MIA': (25.77452579915163, -80.19656914449405),\n",
    "    'PAO': (37.416065, -122.13571963362166),\n",
    "    'PIT': (40.44177902989321, -80.01294377242584),\n",
    "    'WDC': (38.889377, -77.0355047439081)\n",
    "}\n",
    "\n",
    "EARTH_RADIUS = 6378.137\n",
    "CONST_METER_TO_LATLON = 1/(math.pi/180*EARTH_RADIUS*1000)\n",
    "\n",
    "MIN_DISTANCE = 1.0\n",
    "MAX_DISTANCE = 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def meter_to_latlon(city:str, x:float, y:float):\n",
    "#     lat = CITY_ORIGIN_LATLON[city][0] + y * CONST_METER_TO_LATLON\n",
    "#     lon = CITY_ORIGIN_LATLON[city][1] + x * CONST_METER_TO_LATLON / math.cos(lat * math.pi / 180)\n",
    "#     return (lat, lon)\n",
    "\n",
    "def meter_to_latlon(city:str, coords:np.ndarray)->np.ndarray:\n",
    "    lat, lon = CITY_ORIGIN_LATLON[city]\n",
    "    lat = lat + coords[:,1] * CONST_METER_TO_LATLON\n",
    "    lon = lon + coords[:,0] * CONST_METER_TO_LATLON / np.cos(lat*math.pi/180)\n",
    "    return np.stack([lat,lon], axis=1)\n",
    "\n",
    "def get_feature_collection(features):\n",
    "    feature_collection = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": features\n",
    "    }\n",
    "    \n",
    "    return feature_collection\n",
    "\n",
    "def get_timestamp_from_path(path:str)->int:\n",
    "    filename = os.path.basename(path)\n",
    "    return int(filename.split('.')[0])\n",
    "\n",
    "def get_rotation_matrix(w:float, x:float, y:float, z:float)->np.ndarray:\n",
    "    return Rotation([x, y, z, w]).as_matrix()\n",
    "\n",
    "def get_transform_matrix(w:float, x:float, y:float, z:float, tx:float, ty:float, tz:float)->np.ndarray:\n",
    "    mat = np.eye(4)\n",
    "    mat[:3,:3] = get_rotation_matrix(w, x, y, z)\n",
    "    mat[:,3] = [tx, ty, tz, 1.0]\n",
    "    \n",
    "    return mat\n",
    "    \n",
    "def create_points(num_points)->np.ndarray:\n",
    "    return np.zeros((num_points, len(POINT_ATTR)))\n",
    "\n",
    "def transform_points(points:np.ndarray, calibration:np.ndarray):\n",
    "    size = points.shape[0]\n",
    "    coords = np.concatenate((points[:,:3].T, np.ones((1, size))))\n",
    "    coords = np.matmul(calibration, coords)\n",
    "    \n",
    "    return coords[:3,:].T\n",
    "\n",
    "def image_to_numpy(path:str):\n",
    "    image = Image.open(path)\n",
    "    np_image = np.array(image)\n",
    "    \n",
    "    return np_image.transpose(2, 0, 1)\n",
    "\n",
    "# transform points from world to camera pixel\n",
    "def point_to_pixel(points:np.ndarray, to_sensor_calib:np.ndarray, intrinsic:np.ndarray):\n",
    "    coords = transform_points(points, to_sensor_calib)\n",
    "    size = coords.shape[0]\n",
    "    depth = coords[:,2]\n",
    "    \n",
    "    pixel = np.matmul(intrinsic, coords.T)\n",
    "    pixel = pixel / pixel[2, :]\n",
    "\n",
    "    return pixel.T, depth\n",
    "\n",
    "def get_rgb_bilinear(img:np.ndarray, pixel:np.ndarray):\n",
    "    x = pixel[:,0]\n",
    "    y= pixel[:,1]\n",
    "    \n",
    "    x0 = np.floor(x).astype(np.int32)\n",
    "    y0 = np.floor(y).astype(np.int32)\n",
    "    x1 = np.ceil(x).astype(np.int32)\n",
    "    y1 = np.ceil(y).astype(np.int32)\n",
    "\n",
    "    lt = img[:, y0, x0]\n",
    "    lb = img[:, y1, x0]\n",
    "    rt = img[:, y0, x1]\n",
    "    rb = img[:, y1, x1]\n",
    "    \n",
    "    wa = np.expand_dims(((x1.astype(np.float16) - x) * (y1.astype(np.float16) - y)), 0)\n",
    "    wb = np.expand_dims(((x1.astype(np.float16) - x) * (y - y0.astype(np.float16))), 0)\n",
    "    wc = np.expand_dims(((x - x0.astype(np.float16)) * (y1.astype(np.float16) - y)), 0)\n",
    "    wd = np.expand_dims(((x - x0.astype(np.float16)) * (y - y0.astype(np.float16))), 0)\n",
    "    \n",
    "    return (lt * wa + lb * wb + rt * wc + rb * wd).transpose(1, 0)\n",
    "\n",
    "def get_color_from_image(image:str, pixel:np.ndarray, depth:np.ndarray, min_height=1, max_height=0):\n",
    "    np_image = image_to_numpy(image)\n",
    "    _, h, w = np_image.shape\n",
    "    size = pixel.shape[0]\n",
    "    \n",
    "    if min_height > max_height:\n",
    "        max_height = h\n",
    "    \n",
    "    x = pixel[:, 0]\n",
    "    y = pixel[:, 1]\n",
    "    mask = np.ones(size, dtype=bool)\n",
    "    mask = np.logical_and(mask, depth > 0)\n",
    "    mask = np.logical_and(mask, x > 1)\n",
    "    mask = np.logical_and(mask, x < w - 1)\n",
    "    mask = np.logical_and(mask, y > min_height)\n",
    "    mask = np.logical_and(mask, y < max_height - 1)\n",
    "    rgb = get_rgb_bilinear(np_image, pixel[mask,:2])\n",
    "    \n",
    "    return rgb, mask\n",
    "\n",
    "def draw_points_on_image(image:str, pixel:np.ndarray):\n",
    "    np_image = image_to_numpy(image).transpose(1, 2, 0)\n",
    "    x = pixel[:,0]\n",
    "    y = pixel[:,1]\n",
    "    \n",
    "    plt.imshow(np_image)\n",
    "    plt.scatter(x, y, c='blue', marker='x', s=1)\n",
    "    plt.show()\n",
    "    \n",
    "def write_las(path:str, points:np.ndarray):\n",
    "    header = laspy.LasHeader(point_format=3, version=\"1.4\")\n",
    "    header.offsets = np.min(points[:,:3], axis=0)\n",
    "    header.scales = np.array([0.01, 0.01, 0.01])\n",
    "    # header.vlrs\n",
    "    \n",
    "    points_type = points.shape[1]\n",
    "    \n",
    "    las = laspy.LasData(header)\n",
    "    las.x = points[:,0]\n",
    "    las.y = points[:,1]\n",
    "    las.z = points[:,2]\n",
    "    las.intensity = points[:,POINT_ATTR['INTENSITY']]\n",
    "    if points_type >= POINT_ATTR['GPS']:\n",
    "        las.gps_time = points[:,POINT_ATTR['GPS']].astype(np.uint64)\n",
    "    \n",
    "    if points_type >= POINT_ATTR['B']:\n",
    "        las.red = points[:,POINT_ATTR['R']].astype(np.uint8)\n",
    "        las.green = points[:,POINT_ATTR['G']].astype(np.uint8)\n",
    "        las.blue = points[:,POINT_ATTR['B']].astype(np.uint8)\n",
    "    if points_type >= POINT_ATTR['CATEGORY']:\n",
    "        las.classification = points[:,POINT_ATTR['CATEGORY']].astype(np.uint8)\n",
    "    las.write(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EgoPoseParser():\n",
    "    def __init__(self, path:str):\n",
    "        feather = pd.read_feather(path)\n",
    "        \n",
    "        qw = feather['qw']\n",
    "        qx = feather['qx']\n",
    "        qy = feather['qy']\n",
    "        qz = feather['qz']\n",
    "        tx = feather['tx_m']\n",
    "        ty = feather['ty_m']\n",
    "        tz = feather['tz_m']\n",
    "        \n",
    "        self.timestamps = feather['timestamp_ns']\n",
    "        self.ego_calibrations = np.stack(\n",
    "            [get_transform_matrix(qw[i],qx[i],qy[i],qz[i],tx[i],ty[i],tz[i]) for i in range(len(self.timestamps))], axis=0\n",
    "        )\n",
    "        self.coordinates = np.stack([tx, ty, tz], axis=1)\n",
    "    \n",
    "    def get_ego_calibration(self, timestamp: int)->np.ndarray:\n",
    "        return self.ego_calibrations[self.timestamps == timestamp].squeeze()\n",
    "    \n",
    "    def get_coords(self)->np.ndarray:\n",
    "        return self.coordinates\n",
    "                \n",
    "    \n",
    "class CalibrationParser():\n",
    "    def __init__(self, path:str):\n",
    "        feather = pd.read_feather(path)\n",
    "        \n",
    "        qw = feather['qw']\n",
    "        qx = feather['qx']\n",
    "        qy = feather['qy']\n",
    "        qz = feather['qz']\n",
    "        tx = feather['tx_m']\n",
    "        ty = feather['ty_m']\n",
    "        tz = feather['tz_m']\n",
    "        \n",
    "        self.sensor_calibrations = np.stack(\n",
    "            [get_transform_matrix(qw[i],qx[i],qy[i],qz[i],tx[i],ty[i],tz[i]) for i in range(len(qw))], axis=0\n",
    "        )\n",
    "    \n",
    "    def get(self, sensor_id:int)->np.ndarray:\n",
    "        return self.sensor_calibrations[sensor_id].squeeze()\n",
    "        \n",
    "\n",
    "class IntrinsicParser():\n",
    "    def __init__(self, path:str) -> None:\n",
    "        feather = pd.read_feather(path)\n",
    "        \n",
    "        fx_px = feather['fx_px']\n",
    "        fy_px = feather['fy_px']\n",
    "        cx_px = feather['cx_px']\n",
    "        cy_px = feather['cy_px']\n",
    "        \n",
    "        self.intrinsics = np.stack([np.eye(3) for _ in range(len(fx_px))], axis=0)\n",
    "        self.intrinsics[:, 0, 0] = fx_px\n",
    "        self.intrinsics[:, 1, 1] = fy_px\n",
    "        self.intrinsics[:, 0, 2] = cx_px\n",
    "        self.intrinsics[:, 1, 2] = cy_px\n",
    "    \n",
    "    def get(self, sensor_id:int)->np.ndarray:\n",
    "        return self.intrinsics[sensor_id]\n",
    "\n",
    "\n",
    "class LogMapArchive():\n",
    "    def __init__(self, path) -> None:\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.lane_segments = []\n",
    "        self.pedestrian_crossings = []\n",
    "        \n",
    "        # drivable_area\n",
    "        self.drivable_areas = {}\n",
    "        for id in data['drivable_areas']:\n",
    "            coords = []\n",
    "            for boundary in data['drivable_areas'][id]['area_boundary']:\n",
    "                z = 0 if math.isnan(boundary['z']) else boundary['z']\n",
    "                coords.append(np.array([boundary['x'], boundary['y'], z]))\n",
    "            coords = np.stack(coords, axis=0)\n",
    "            self.drivable_areas[id] = coords\n",
    "            \n",
    "    def get_drivable_areas(self)->str:\n",
    "        keys = []\n",
    "        for key, _ in self.drivable_areas.items():\n",
    "            keys.append(key)\n",
    "        \n",
    "        return keys\n",
    "                \n",
    "    def get_drivable_points(self, id:str)->np.ndarray:\n",
    "        coords = self.drivable_areas[id]\n",
    "        points = create_points(coords.shape[0])\n",
    "        points[:,:3] = coords\n",
    "        points[:,POINT_ATTR['INTENSITY']] = 255\n",
    "        \n",
    "        return points\n",
    "\n",
    "\n",
    "class Annotation():\n",
    "    def __init__(self, category:int, length:float, width:float, height:float, transform_mat:np.ndarray, inner_points:int) -> None:\n",
    "        self.category = category\n",
    "        self.length = length/2 + 5e-2\n",
    "        self.width = width/2 + 5e-2\n",
    "        self.height = height/2 + 5e-2\n",
    "        self.transform_mat = np.linalg.inv(transform_mat)\n",
    "        self.inner_points = inner_points\n",
    "    \n",
    "    def classify_points(self, points:np.ndarray) -> None:\n",
    "        size = points.shape[0]\n",
    "        coords = points[:,:3]\n",
    "        coords = transform_points(coords, self.transform_mat)\n",
    "        \n",
    "        xs = coords[:,0]\n",
    "        ys = coords[:,1]\n",
    "        zs = coords[:,2]\n",
    "        \n",
    "        mask = xs >= -self.length\n",
    "        mask = np.logical_and(mask, xs<=self.length)\n",
    "        mask = np.logical_and(mask, ys>=-self.width)\n",
    "        mask = np.logical_and(mask, ys<=self.width)\n",
    "        mask = np.logical_and(mask, zs>=-self.height)\n",
    "        mask = np.logical_and(mask, zs<=self.height)\n",
    "        \n",
    "        categories = np.zeros((size))\n",
    "        categories[mask] = self.category\n",
    "        \n",
    "        diff = self.inner_points - categories[mask].shape[0]\n",
    "        if diff > 0:\n",
    "            print(self.category, diff)\n",
    "        \n",
    "        return categories\n",
    "        \n",
    "\n",
    "class AnnotationParser():\n",
    "    def __init__(self, path:str) -> None:\n",
    "        if not os.path.exists(path):\n",
    "            self.valid = False\n",
    "            return\n",
    "        \n",
    "        feather = pd.read_feather(path)\n",
    "        \n",
    "        timestamps = feather['timestamp_ns']\n",
    "        categories = feather['category']\n",
    "        lengths = feather['length_m']\n",
    "        widths = feather['width_m']\n",
    "        heights = feather['height_m']\n",
    "        qw = feather['qw']\n",
    "        qx = feather['qx']\n",
    "        qy = feather['qy']\n",
    "        qz = feather['qz']\n",
    "        tx = feather['tx_m']\n",
    "        ty = feather['ty_m']\n",
    "        tz = feather['tz_m']\n",
    "        inner_points = feather['num_interior_pts']\n",
    "        \n",
    "        num_data = len(timestamps)\n",
    "        transform_matrix = [get_transform_matrix(qw[i],qx[i],qy[i],qz[i],tx[i],ty[i],tz[i]) for i in range(num_data)]\n",
    "        \n",
    "        self.annotations = {}\n",
    "        for i in range(num_data):\n",
    "            timestamp = timestamps[i]\n",
    "            if not timestamp in self.annotations:\n",
    "                self.annotations[timestamp] = []\n",
    "            self.annotations[timestamp].append(Annotation(CATEGORY[categories[i]], lengths[i], widths[i], heights[i], transform_matrix[i], inner_points[i]))\n",
    "            \n",
    "        self.valid = True\n",
    "            \n",
    "    def get_annotations(self, timestamp:int)->list[Annotation]:\n",
    "        if self.valid:\n",
    "            return self.annotations[timestamp]\n",
    "        return []\n",
    "        \n",
    "\n",
    "\n",
    "class Lidar():\n",
    "    def __init__(self, timestamp:int, path:str, ego_calibration:np.ndarray, ul_calibration:np.ndarray, dl_calibration:np.ndarray, annotations:list[Annotation]):\n",
    "        self.timestamp = timestamp\n",
    "        self.path = path\n",
    "        self.ego_calibration = ego_calibration\n",
    "        self.ul_calibration = ul_calibration\n",
    "        self.dl_calibration = dl_calibration\n",
    "        self.annotations = annotations\n",
    "        self.is_init = False\n",
    "        \n",
    "    def init(self)->None:\n",
    "\n",
    "        feather = pd.read_feather(self.path)\n",
    "        xs = feather['x']\n",
    "        ys = feather['y']\n",
    "        zs = feather['z']\n",
    "\n",
    "        coordinates = np.stack([xs, ys, zs], axis=1)\n",
    "        laser_number = feather['laser_number']\n",
    "        intensities = feather['intensity']\n",
    "        gps_times = feather['offset_ns']\n",
    "        \n",
    "        # # distance filter\n",
    "        # distance = np.sum(np.square(coordinates), axis=1)\n",
    "        # distance_filter = distance <= MAX_DISTANCE**2\n",
    "        # coordinates = coordinates[distance_filter]\n",
    "        # laser_number = laser_number[distance_filter]\n",
    "        # intensities = intensities[distance_filter]\n",
    "        # gps_times = gps_times[distance_filter]\n",
    "        \n",
    "        num_points = coordinates.shape[0]\n",
    "        categories = np.zeros((num_points))\n",
    "        point_index = np.arange(num_points)\n",
    "        \n",
    "        \n",
    "        # # transform coordinates from lidar to world (NEVER USED!)\n",
    "        # up_index = laser_number[laser_number >= 32]\n",
    "        # down_index = laser_number[laser_number < 32]\n",
    "        # # lidar to ego vehicle\n",
    "        # coordinates[up_index] = transform_points(coordinates[up_index], self.dl_calibration)\n",
    "        # coordinates[down_index] = transform_points(coordinates[down_index], self.ul_calibration)\n",
    "        \n",
    "        # classify points with annotations\n",
    "        if CLASSIFY_CATEGORY:\n",
    "            for annotation in self.annotations:\n",
    "                classified = annotation.classify_points(coordinates)\n",
    "                valid = classified > 0\n",
    "                categories[valid] = classified[valid]\n",
    "\n",
    "        # ego vehicle to world\n",
    "        coordinates = transform_points(coordinates, self.ego_calibration)\n",
    "        \n",
    "        self.coordinates = coordinates\n",
    "        self.intensities = intensities\n",
    "        self.gps_times = gps_times + self.timestamp\n",
    "        self.categories = categories\n",
    "        self.point_index = point_index\n",
    "        self.is_init = True\n",
    "    \n",
    "    # x, y, z, intensity, gps_time, point-index\n",
    "    def get_points(self)->np.ndarray:\n",
    "        if not self.is_init:\n",
    "            self.init()\n",
    "        \n",
    "        points = create_points(self.coordinates.shape[0])\n",
    "        points[:,:3] = self.coordinates\n",
    "        points[:,POINT_ATTR['INTENSITY']] = self.intensities\n",
    "        points[:,POINT_ATTR['GPS']] = self.gps_times\n",
    "        points[:,POINT_ATTR['INDEX']] = self.point_index\n",
    "        points[:,POINT_ATTR['CATEGORY']] = self.categories\n",
    "        \n",
    "        return points\n",
    "\n",
    "class Camera():\n",
    "    def __init__(self, timestamp:int, images:list[str], ego_calibration:np.ndarray, sensor_calibrations:CalibrationParser, intrinsics:IntrinsicParser) -> None:\n",
    "        self.timestamp = timestamp\n",
    "        self.images = images\n",
    "        self.ego_calibraion = ego_calibration\n",
    "        self.sensor_calibrations = sensor_calibrations\n",
    "        self.intrinsics = intrinsics\n",
    "        \n",
    "    def paint_points(self, points:np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        painted_points = []\n",
    "        # world to vehicle\n",
    "        coords = transform_points(points, np.linalg.inv(self.ego_calibraion))\n",
    "        \n",
    "        for _, item in SENSOR.items():\n",
    "            sensor_id = item['id']\n",
    "            if sensor_id >= SENSOR['SFL']['id']:\n",
    "                continue\n",
    "\n",
    "            valid_height = item['valid_height'] if VALID_HEIGHT_ONLY else [1, 0]\n",
    "            image = self.images[sensor_id]\n",
    "            cam_calib = self.sensor_calibrations.get(sensor_id)\n",
    "            intrinsic = self.intrinsics.get(sensor_id)\n",
    "            # project points into camera\n",
    "            pixel, depth = point_to_pixel(coords, np.linalg.inv(cam_calib), intrinsic)\n",
    "            rgb, mask = get_color_from_image(image, pixel, depth, valid_height[0], valid_height[1])\n",
    "            points[mask,POINT_ATTR['R']:POINT_ATTR['R']+3] = rgb\n",
    "            \n",
    "            painted_points.append(points[mask])\n",
    "            points = points[np.logical_not(mask)]\n",
    "            coords = coords[np.logical_not(mask)]\n",
    "            \n",
    "            if DEBUG_MODE:\n",
    "                draw_points_on_image(image, pixel[mask])\n",
    "                \n",
    "        points = np.concatenate(painted_points, 0)\n",
    "        \n",
    "        return points\n",
    "    \n",
    "\n",
    "class Argoverse():\n",
    "    def __init__(self, root:str, id:str) -> None:\n",
    "        self.id = id\n",
    "        self.root = os.path.join(root, id)\n",
    "        \n",
    "        # parse map\n",
    "        map_files = glob.glob(join(self.root, 'map', '*'))\n",
    "        for file in map_files:\n",
    "            if 'log_map_archive' in file:\n",
    "                pattern = r'____(.*?)_city'\n",
    "                match = re.search(pattern, file)\n",
    "                if match:\n",
    "                    self.city = match.group(1)\n",
    "                    \n",
    "                self.log_map_archive = LogMapArchive(file)\n",
    "        \n",
    "        # parse sensors\n",
    "        self.sensor_calibrations = CalibrationParser(join(self.root, 'calibration', 'egovehicle_SE3_sensor.feather'))\n",
    "        self.intrinsics = IntrinsicParser(join(self.root, 'calibration', 'intrinsics.feather'))\n",
    "        \n",
    "        annotations = AnnotationParser(join(self.root, 'annotations.feather'))\n",
    "        self.ego_poses = EgoPoseParser(join(self.root, 'city_SE3_egovehicle.feather'))\n",
    "        self.sensors = self.sync_lidar_camera(self.parse_lidars(annotations), self.parse_cameras())\n",
    "    \n",
    "    def parse_cameras(self)->list[Camera]:\n",
    "        num_images = 10e4\n",
    "        cameras = []\n",
    "        num_cams = SENSOR['SR']['id'] + 1\n",
    "        image_files = [[] for _ in range(num_cams)]\n",
    "        \n",
    "        # get all jpg from each sensors dir\n",
    "        for _, item in SENSOR.items():\n",
    "            if item['id'] >= num_cams:\n",
    "                continue\n",
    "            image_files[item['id']] = glob.glob(join(self.root, 'sensors', 'cameras', item['name'], '*.jpg'))\n",
    "            num_images = min(num_images, len(image_files[item['id']]))\n",
    " \n",
    "        # create Camera instances\n",
    "        for i in range(num_images):\n",
    "            timestamp = get_timestamp_from_path(image_files[0][i])\n",
    "            ego_calibration = self.ego_poses.get_ego_calibration(timestamp)\n",
    "            images= []\n",
    "            for j in range(num_cams):\n",
    "                images.append(image_files[j][i])\n",
    "            \n",
    "            cameras.append(Camera(timestamp, images, ego_calibration, self.sensor_calibrations, self.intrinsics))\n",
    "        \n",
    "        return cameras\n",
    "\n",
    "        \n",
    "    def parse_lidars(self, annotations:AnnotationParser)->list[Lidar]:\n",
    "        ul_calibration = self.sensor_calibrations.get([SENSOR['UL']['id']])\n",
    "        dl_calibration = self.sensor_calibrations.get([SENSOR['DL']['id']])\n",
    "        \n",
    "        lidar_files = glob.glob(join(self.root, 'sensors', 'lidar', '*.feather'))\n",
    "        lidars = []\n",
    "        for lidar_file in lidar_files:\n",
    "            timestamp = get_timestamp_from_path(lidar_file)\n",
    "            ego_calibration = self.ego_poses.get_ego_calibration(timestamp)\n",
    "            lidars.append(Lidar(timestamp, lidar_file, ego_calibration, ul_calibration, dl_calibration, annotations.get_annotations(timestamp)))\n",
    "            \n",
    "        return lidars\n",
    "    \n",
    "    def sync_lidar_camera(self, lidars:list[Lidar], cameras:list[Camera])->list(tuple((Lidar, Camera))):\n",
    "        sensors = []\n",
    "\n",
    "        for lidar in lidars:\n",
    "            timestamp = lidar.timestamp\n",
    "            \n",
    "            for camera in cameras:\n",
    "                if timestamp < camera.timestamp:\n",
    "                    sensors.append((lidar, camera))\n",
    "                    break\n",
    "        \n",
    "        return sensors\n",
    "\n",
    "    \n",
    "    def get_points(self)->np.ndarray:\n",
    "        num_points = 0\n",
    "        points_list = []\n",
    "        \n",
    "        for sensors in self.sensors:\n",
    "            lidar:Lidar = sensors[0]\n",
    "            points = lidar.get_points()\n",
    "            points[:,POINT_ATTR['INDEX']] += num_points\n",
    "            num_points += points[-1,POINT_ATTR['INDEX']]\n",
    "            \n",
    "            camera:Camera = sensors[1]\n",
    "            points = camera.paint_points(points)\n",
    "            points_list.append(points)\n",
    "        \n",
    "        # points = self.log_map_archive.get_drivable_points()\n",
    "        # points[:,POINT_ATTR['CATEGORY']] = 7\n",
    "        # points_list.append(points)\n",
    "        \n",
    "        return np.concatenate(points_list, axis=0)\n",
    "    \n",
    "    # Lat Lon coords system\n",
    "    def get_feature(self):\n",
    "        feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": {\n",
    "                \"type\": \"LineString\",\n",
    "                \"coordinates\": meter_to_latlon(self.city, self.ego_poses.coordinates)\n",
    "            },\n",
    "            \"properties\":{\n",
    "                'id': self.id,\n",
    "                'city': self.city\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'MOBILE_PEDESTRIAN_CROSSING_SIGN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m features \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m ids:\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     features\u001b[39m.\u001b[39mappend(Argoverse(root, \u001b[39mid\u001b[39;49m)\u001b[39m.\u001b[39mget_feature())\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdata.geojson\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m json_file:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(get_feature_collection(features), json_file)\n",
      "\u001b[1;32m/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=304'>305</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msensor_calibrations \u001b[39m=\u001b[39m CalibrationParser(join(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39m'\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39megovehicle_SE3_sensor.feather\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=305'>306</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintrinsics \u001b[39m=\u001b[39m IntrinsicParser(join(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39m'\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mintrinsics.feather\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=307'>308</a>\u001b[0m annotations \u001b[39m=\u001b[39m AnnotationParser(join(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, \u001b[39m'\u001b[39;49m\u001b[39mannotations.feather\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=308'>309</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mego_poses \u001b[39m=\u001b[39m EgoPoseParser(join(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39m'\u001b[39m\u001b[39mcity_SE3_egovehicle.feather\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=309'>310</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msensors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msync_lidar_camera(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_lidars(annotations), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_cameras())\n",
      "\u001b[1;32m/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=161'>162</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m timestamp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mannotations:\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mannotations[timestamp] \u001b[39m=\u001b[39m []\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mannotations[timestamp]\u001b[39m.\u001b[39mappend(Annotation(CATEGORY[categories[i]], lengths[i], widths[i], heights[i], transform_matrix[i], inner_points[i]))\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yeti/pointcloud_correction/test/ArgoverseSensor.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MOBILE_PEDESTRIAN_CROSSING_SIGN'"
     ]
    }
   ],
   "source": [
    "root = '/mnt/d/argoverse/WDC'\n",
    "ids = os.listdir(root)\n",
    "features = []\n",
    "for id in ids:\n",
    "    features.append(Argoverse(root, id).get_feature())\n",
    "\n",
    "with open('data.geojson', 'w') as json_file:\n",
    "    json.dump(get_feature_collection(features), json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/d/argoverse/WDC'\n",
    "id = '01bb304d-7bd8-35f8-bbef-7086b688e35e'\n",
    "argoverse = Argoverse(path, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argoverse.get_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = argoverse.log_map_archive.get_drivable_areas()\n",
    "# for id in ids:\n",
    "#     points = argoverse.log_map_archive.get_drivable_points(id)\n",
    "#     points[:,POINT_ATTR['INDEX']] = np.arange(points.shape[0])\n",
    "#     points[:,POINT_ATTR['INTENSITY']] = 254\n",
    "#     write_las(join(path, f'{id}.las'), points)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_las(join(path, 'test.las'), argoverse.get_points())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_feather(os.path.join(path, 'sensors', 'lidar', '315968867659956000.feather'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py309",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
